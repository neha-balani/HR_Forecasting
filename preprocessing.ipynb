{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Preparation](#Preparation)\n",
    "    1. [Define S3 Parameters](#DefineS3Parameters)\n",
    "    1. [Import Python Libraries](#ImportPythonLibraries)\n",
    "    1. [Define Code to Export Data (Optional)](#ExportDataCode)\n",
    "1. [Read Data](#ReadData)\n",
    "1. [Transformation](#Transformation)\n",
    "    1. [Initial Feature Selection](#FeatureSelection)\n",
    "    1. [Handling missing values](#MissignValues)\n",
    "    1. [Formatting Dataset](#FormattingDataset)\n",
    "    1. [Feature Engineering](#FeatureEngineering)\n",
    "    1. [Filter and Sample Data](#FilterSampleData)\n",
    "    1. [Outlier Treatment](#OutlierTreatment)\n",
    "1. [Data Agumentation](#DataAugmentation)\n",
    "    1. [Oddly distributed data](#OddlyDistributedData)\n",
    "1. [Data Conversion](#DataConversion)      \n",
    "    1. [Converting categorical to numeric](#Categorical2Numeric) \n",
    "    1. [One Hot Encoding (Dummy variables)](#OneHotEncoding)\n",
    "1. [Data Normalization](#DataNormalization) \n",
    "1. [preprocessing.py File](#preprocessing.pyFile)\n",
    "\n",
    "---\n",
    "<a id='Preparation'></a>\n",
    "## Preparation\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='DefineS3Parameters'></a>\n",
    "### Define S3 Parameters\n",
    "From the initial dataset, select the features to be worked with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "s3_client = boto3.client('s3')                     # 's3_client' is a key word. create connection to S3 using default config and all buckets within S3\n",
    "\n",
    "# Define File Paths and S3 Buckets\n",
    "bucket_name = '<SiemensProjectBucket>'\n",
    "prefix = 'preprocessing'\n",
    "\n",
    "file_key = \"<folder>/<InputFile>.csv\"\n",
    "\n",
    "s3_output_location = 's3://{}/{}'.format(bucket_name, prefix)\n",
    "print('training artifacts will be uploaded to: {}'.format(s3_output_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ImportPythonLibraries'></a>\n",
    "### Import Python Libraries\n",
    "\n",
    "Now let's bring in the Python libraries that we'll use throughout the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "import numpy as np                                        # For matrix operations and numerical processing\n",
    "import pandas as pd                                       # For munging tabular data\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt                           # For charts and visualizations\n",
    "from IPython.display import Image                         # For displaying images in the notebook\n",
    "from IPython.display import display                       # For displaying outputs in the notebook\n",
    "from time import gmtime, strftime                         # For labeling SageMaker models, endpoints, etc.\n",
    "#from imblearn.under_sampling import RandomUnderSampler   # For using undersampling in dataframes ---------> Library imblearn not Installed. Include in requirements.txt\n",
    "#from imblearn.over_sampling import SMOTE                 # For using oversampling in dataframes  ---------> Library imblearn not Installed. Include in requirements.txt\n",
    "import sys                                                # For writing outputs to notebook\n",
    "import math                                               # For ceiling function\n",
    "import json                                               # For parsing hosting outputs\n",
    "import os                                                 # For manipulating filepath names\n",
    "\n",
    "class AwsUtility():\n",
    "    @staticmethod\n",
    "    def read_csv_df(file_key)\n",
    "        df = pd.read_csv('s3://{}/{}'.format(bucket_name, file_key))\n",
    "        print ('{} rows have been read in from AWS S3.'.format(len(df)))\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def read_pickle(file_key):\n",
    "        obj = client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "        data = pickle.loads(obj['Body'].read())\n",
    "        print ('{} rows have been read in from AWS S3.'.format(len(data)))\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def upload_csv_to_s3(df, file_key):\n",
    "        csv_buffer = StringIO(),\n",
    "        df.to_csv(csv_buffer, sep=','),\n",
    "        obj = s3.Object(bucket_name, '{}/{}'.format(prefix, file_key),\n",
    "        obj.put(\n",
    "            Body=csv_buffer.getvalue(),\n",
    "            ServerSideEncryption='aws:kms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='ReadData'></a>\n",
    "## Read Data\n",
    "Let's start by reading the data previously stored in your project S3 and convert it into a Pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files (key) from S3 Buckets and write them into df format\n",
    "initial_df_1 = read_csv_df('<prefix/source_file_name>')\n",
    "initial_df_2 = read_csv_df('<prefix/source_file_name>') \n",
    "\n",
    "# Load files (key) from S3 Buckets and write them into df format\n",
    "data = read_pickle('<prefix/source_file_name>')\n",
    "\n",
    "# Combine Data Frames - if applicable\n",
    "frames = [initial_df_1, initial_df_2]\n",
    "df = pd.concat(frames)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns in dataset are the Following:\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Null values in Received_Date variable are the following:\")\n",
    "df.<ColumnName>.isna().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='Transformation'></a>\n",
    "## Transformation\n",
    "\n",
    "The goal in this section is to apply all transformations necessary to clean data, transform it, and filter if necessary.\n",
    "Cleaning up data is part of nearly every advanced analytics project. It arguably presents the biggest risk if done incorrectly and is one of the more subjective aspects in the process.  \n",
    "\n",
    "A key rationale in this section is to apply transformations while keeping changes in data visible from a business sense point of view. That is \n",
    "- On every step we should be able to apply changes so that we can export and visualize data.\n",
    "- Interpretability of data should be assured. Thus we will be able to forsee business sence being applied in data and afterwards fed into the modeling. \n",
    "\n",
    "\n",
    "\n",
    "<a id='FeatureSelection'></a>\n",
    "### Initial Feature Selection\n",
    "From the initial dataset, select the features to be worked with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data_model(data):\n",
    "    model_data = data.copy().rename(columns={    #rename columns\n",
    "        'col_name_old_1':'col_name_new_1',\n",
    "        'col_name_old_2':'col_name_new_2'\n",
    "        }).reindex(                          #drop any unwanted columns\n",
    "        columns=['col_name_new_1', 'col_name_new_2']))\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Model with features preprocessed\n",
    "model = select_data_model(data)\n",
    "print(\"Data Point Count in this step is: {}\".format(len(model)))\n",
    "\n",
    "model.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='MissignValues'></a>\n",
    "### Handling missing values\n",
    "Some algorithms are capable of handling missing values, but most would rather not.  Options include:\n",
    " * Removing observations with missing values: This works well if only a very small fraction of observations have incomplete information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_treatment(data):\n",
    "    model_data = pd.DataFrame(data)\n",
    "    model_data = model_data.replace('-', np.nan)\n",
    "    model_data = model_data.dropna()\n",
    "\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = missing_treatment(model)\n",
    "\n",
    "print(\"Data Point Count in this step is: {}\".format(len(model)))\n",
    "model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Imputing missing values: Entire [books](https://www.amazon.com/Flexible-Imputation-Missing-Interdisciplinary-Statistics/dp/1439868247) have been written on this topic, but common choices are replacing the missing value with the mode or mean of that column's non-missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['<column_name>'].fillna(df['<column_name>'].mode()) # To replace NaN values by the column mode\n",
    "\n",
    "df['<column_name>'].fillna(df['<column_name>'].mean()) # To replace NaN values by the column mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='FormattingDataset'></a>\n",
    "### Formatting Dataset\n",
    "Some data needs to be formatted in the correct way for processing. This is especially true for Time variables. In some cases, we even need to order our datapoints, like in time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_model(data):\n",
    "    model_data = pd.DataFrame(data)\n",
    "    model_data = model_data[1:]\n",
    "    model_data['ReceivedDate'] = pd.to_datetime(model_data['ReceivedDate'], dayfirst=True, errors='coerce')\n",
    "    model_data['Date'] = pd.to_datetime(model_data['Date'], dayfirst=True, errors='coerce')\n",
    "    model_data['Col1'] = model_data['Col1'].astype(np.int64)\n",
    "    model_data['DateMonth'] = model_data['Date'].dt.month\n",
    "    \n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = format_model(model)\n",
    "\n",
    "print(\"Data Point Count in this step is: {}\".format(len(model)))\n",
    "model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='FeatureEngineering'></a>\n",
    "## Feature Engineering\n",
    "Another question to ask yourself before building a model is whether certain features will add value in your final use case.  For example, if your goal is to deliver the best prediction, then will you have access to that data at the moment of prediction? If not, can you somehow forecast them accurately?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Pseudo Code Example ########################\n",
    "def FeatureEngineeringFunction(model_data):\n",
    "    ####### Combine variables\n",
    "    ####### reduce categories in some variables\n",
    "    ####### etc\n",
    "    ####### etc\n",
    "    \n",
    "    ####### delete previous variables that will be obselote after feature engineering\n",
    "    ####### ...\n",
    "    pass\n",
    "######################## Pseudo Code Example ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = FeatureEngineeringFunction(model)\n",
    "\n",
    "df.columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='FilterSampleData'></a>\n",
    "### Filter and Sample Data\n",
    "Commonly there are 2 reasons for filtering data:\n",
    "* For development purposes feed the model with only partial data, for test cases\n",
    "* For structural reasons, if we want the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Pseudo Code Example ########################\n",
    "def DataFileterFunction(model_data):\n",
    "    ### Filter some of your data, whether it will serve just for testing\n",
    "    ### or if you just want to to look at one region or any other dimension\n",
    "    ###\n",
    "    \n",
    "    ###\n",
    "    pass\n",
    "######################## Pseudo Code Example ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_updated = DataFileterFunction(df)\n",
    "\n",
    "print(\"Data Filtered:\")\n",
    "data_updated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='OutlierTreatment'></a>\n",
    "### Outlier Treatment\n",
    "This step is focused on the treatment of outliers. That is, to detect and treat datapoints that deviate too far from the rest of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Removing features with missing values: This works well if there are a small number of features which have a large number of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(data):\n",
    "    outliers = stats.zscore(data['col3'])               \n",
    "    squarer = lambda x: np.abs(x) < 2.5\n",
    "    vfunc = np.vectorize(squarer)\n",
    "    updated_data = data[vfunc(outliers)]\n",
    "    \n",
    "    return updated_data[vfunc(outliers_lt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = remove_outliers(data_updated)\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='DataAugmentation'></a>\n",
    "## **3. Data Augmentation**\n",
    "\n",
    "Data Augmentation can be used in several ways, and thus, it can appear either in Data Preprocessing or Modelling Notebooks. The rationale is the following:\n",
    "\n",
    "   **1 - Preprocessing:** When augmentation of data can explicitly be used to enhance the Dataset. Example: Dealing with imbalanced datasets (Under or Oversampling).\n",
    "   \n",
    "   **2 - Modelling:** When augmentation of data is done implicitly during the model training. Example: When data loaders are used as input for a CNN (in Tensorflow or PyTorch). These data loaders usually have the option of agumenting image data, for instance mirroring the input images.\n",
    "\n",
    "\n",
    "Naturally, as we are using the **Pre-processing Notebook**, this step is used to establish ways of augmenting data explicitly. Several Techniques can be applied in this regard.\n",
    "\n",
    "\n",
    "<a id='OddlyDistributedData'></a>\n",
    "### Oddly distributed data\n",
    "Although for non-linear models like Gradient Boosted Trees, this has very limited implications, parametric models like regression can produce wildly inaccurate estimates when fed highly skewed data. Some of the options are:\n",
    "\n",
    "- For the most simple cases, taking the natural log of the features is sufficient to produce more normally distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['<new_column_name>'] = np.log(df['column_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In more complex scenarios, bucketing values into discrete ranges is helpful. These buckets can then be treated as categorical variables and included in the model when one hot encoded as explained on the [Converting categorical to numeric](#Categorical2Numerical) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_labels = ['first_label', 'second_label', 'third_label', 'fourth_label']\n",
    "\n",
    "df['quantile_qcut'] = pd.qcut(df['column_name'], q=<number of quantils>, labels=bin_labels) # Quantile-based discretization function: to divide up the underlying data into equal sized bins\n",
    "\n",
    "cut_bins = [<first_boundary>, <second_boundary>, <third_boundary>, <fourth_boundary>]\n",
    "df['quantile_cut'] = pd.cut(df['column_name'], q=cut_bins, labels=bin_labels) # Standard discretization function: to divide up the underlying data between manually specified bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the previous are not enough, we might need to do some downsampling or upsampling (depending on the data volume we have at our disposal) to obtain a balanced enough trainning set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downsampling\n",
    "rus = RandomUnderSampler(return_indices=True)\n",
    "X_rus, y_rus, id_rus = rus.fit_sample(<features_dataset>, <output_vector>)\n",
    "print('Removed indexes: ', id_rus)\n",
    "\n",
    "#Upsampling (via SMOTE)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_sm, y_sm = smote.fit_sample(<features_dataset>, <output_vector>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='DataConversion'></a>\n",
    "## Data Conversion\n",
    "Depending on the model our data is going to feed, some data variables need to be converted, namely from categorical to numerical.\n",
    "This is necessary for Neural Network or XGBoost, for instance. Some other models (like several decision trees based models) might not nee this step.\n",
    "\n",
    "\n",
    "<a id='Categorical2Numeric'></a>\n",
    "### Converting categorical to numeric\n",
    "Sometimes, data can be converted into numerical, if the categories reflect some ordinal notion (In this case, one categorical column is transformed into another with ordinal numbers reflecting the categories rank order).\n",
    "\n",
    "Example:\n",
    "Terrible - 1\n",
    "Poor     - 2\n",
    "Medium   - 3\n",
    "Good     - 4\n",
    "Excelent - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Pseudo Code Example ########################\n",
    "def DataAugmentationFunction:\n",
    "    ### Define ways to augment\n",
    "    ### Example: use the Smote algorithm for imbalanced datasets\n",
    "    ###\n",
    "\n",
    "    ###\n",
    "    pass\n",
    "\n",
    "######################## Pseudo Code Example ########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='OneHotEncoding'></a>\n",
    "### One Hot Encoding (Dummy variable Creation)\n",
    "Most common method to feed Data modelling, especially necessary for XGBoost and NN implementations.\n",
    "\n",
    "For each feature maps (categorical variables), all distinct categorical values will have their own (binary) variable - with value 1 if that category applies, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encode Variables (Dummy Variable Creation)\n",
    "def OneHotEncode_AllCategVar(DataFrame):\n",
    "    List_AllColumns = DataFrame.columns\n",
    "    List_NumColumns = DataFrame._get_numeric_data().columns\n",
    "    List_CategColumns = list(set(List_AllColumns) - set(List_NumColumns))\n",
    "    NewDataFrame = pd.get_dummies(DataFrame, columns = List_CategColumns, drop_first = True)\n",
    "    \n",
    "    return NewDataFrame\n",
    "\n",
    "\n",
    "model_data = OneHotEncode_AllCategVar(model_data)\n",
    "\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='DataNormalization'></a>\n",
    "## Data Normalization\n",
    "Now that we have all the features we want to include, we need to normalize data, so our model can use it correctly:\n",
    "* Especially in numerical data, normalization is essential so that some variables won't be dominant in relation to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataframe):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.column3 = normalize(gross_order_value_model)\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Pre-processed Data to S3\n",
    "\n",
    "Making it available for the modelling step.\n",
    "\n",
    "(Same function defined at the beggining for data export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Export Pre-processed data to S3, for further action on Modelling step ####\n",
    "upload_csv_to_s3(df, '<prefix/source_file_name>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove local files at the end of export step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_command = \"rm -rf ./{}\".format(filename)\n",
    "!$remove_command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='preprocessing.pyFile'></a>\n",
    "## _preprocessing.py_ File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing code used in previous cells should be condensed and runned in the magic cell bellow. After that it should be commited and pushed to the project repo for further use in our data science pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --output-dir='../04-deployment/' --to script preprocessing.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}